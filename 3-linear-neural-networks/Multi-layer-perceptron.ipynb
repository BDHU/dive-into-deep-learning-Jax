{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8768f5ce",
   "metadata": {},
   "source": [
    "## Follow the [Guide](https://roberttlange.github.io/posts/2020/03/blog-post-10/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bf861",
   "metadata": {},
   "source": [
    "### vmap demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6c05f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 100)\n",
      "(32, 100)\n",
      "(512,)\n",
      "dot product shape\n",
      "(512,)\n",
      "(512,)\n",
      "(32, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "from jax import random\n",
    "\n",
    "# Generate key which is used to generate random numbers\n",
    "key = random.PRNGKey(1)\n",
    "\n",
    "batch_dim = 32\n",
    "feature_dim = 100\n",
    "hidden_dim = 512\n",
    "\n",
    "# Generate a batch of inputs\n",
    "X = random.normal(key, (batch_dim, feature_dim))\n",
    "\n",
    "# Generate Gaussian weights and biases\n",
    "params = [random.normal(key, (hidden_dim, feature_dim)),\n",
    "          random.normal(key, (hidden_dim,))]\n",
    "\n",
    "W = params[0]\n",
    "print(W.shape)\n",
    "print(X.shape)\n",
    "b = params[1]\n",
    "print(b.shape)\n",
    "\n",
    "def ReLU(x):\n",
    "    \"\"\" Rectified Linear Unit (ReLU) activation function \"\"\"\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def ReLU_Layer(W, x, b):\n",
    "    return ReLU(jnp.dot(W, x) + b)\n",
    "\n",
    "def vmap_ReLU_Layer(func):\n",
    "    return jit( vmap(func, in_axes=(None, 0, None), out_axes=(0)) )\n",
    "print(\"dot product shape\")\n",
    "print(jnp.dot(W, X[0]).shape)\n",
    "print((jnp.dot(W, X[0]) + b).shape)\n",
    "\n",
    "relu = vmap_ReLU_Layer(ReLU_Layer)\n",
    "result = relu(W, X, b)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee0b5dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "def relu_layer(params, x):\n",
    "    \"\"\" Simple ReLu layer for single sample \"\"\"\n",
    "    return ReLU(np.dot(params[0], x) + params[1])\n",
    "\n",
    "def batch_version_relu_layer(params, x):\n",
    "    \"\"\" Error prone batch version \"\"\"\n",
    "    return ReLU(np.dot(X, params[0].T) + params[1])\n",
    "\n",
    "def vmap_relu_layer(params, x):\n",
    "    \"\"\" vmap version of the ReLU layer \"\"\"\n",
    "    return jit(vmap(relu_layer, in_axes=(None, 0), out_axes=0))\n",
    "\n",
    "out = jnp.stack([relu_layer(params, X[i, :]) for i in range(X.shape[0])])\n",
    "out = batch_version_relu_layer(params, X)\n",
    "out = vmap_relu_layer(params, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4656af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
