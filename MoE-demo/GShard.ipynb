{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef63864",
   "metadata": {},
   "source": [
    "## GShard Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96d496",
   "metadata": {},
   "source": [
    "### Group-level top-2 gating with auxiliary loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4930f6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sharded input shape: (4, 32, 128)\n",
      "wg shape: (128, 4)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "# set up basic params\n",
    "\n",
    "# S = N/G, where N is the number of tokens in a training batch,\n",
    "# G is the number of groups, S is the number of tokens per group\n",
    "num_groups = 4 # G\n",
    "num_experts = 4 # E\n",
    "num_features = 128 # M\n",
    "capacity_factor = 1.2\n",
    "batch_size = 128\n",
    "num_devices = 2 # D\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "batched_inputs = jax.random.normal(key, shape=(batch_size, num_features), dtype=jnp.float32)\n",
    "\n",
    "## sharding inputs\n",
    "sharded_inputs = jnp.reshape(batched_inputs,\n",
    "                             newshape=(num_groups, jnp.floor_divide(batch_size, num_groups), num_features))\n",
    "print(\"sharded input shape: {}\".format(sharded_inputs.shape))\n",
    "\n",
    "## init wg\n",
    "# gates per token per expert\n",
    "wg = jax.random.normal(key, shape=(num_features, num_experts), dtype=jnp.float32)\n",
    "print(\"wg shape: {}\".format(wg.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50d9a8",
   "metadata": {},
   "source": [
    "## gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b06e48de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 4)\n"
     ]
    }
   ],
   "source": [
    "## gates = softmax ( einsum ( \"GSM , ME - >GSE \" , inputs , wg ))\n",
    "\n",
    "local_inputs = sharded_inputs[0]\n",
    "\n",
    "# make group-local by removing G\n",
    "output = jnp.einsum(\"SM,ME->SE\", local_inputs, wg)\n",
    "# pass through softmax\n",
    "gates = jax.nn.softmax(output, axis=1)\n",
    "print(gates.shape)\n",
    "# test the sum is correct\n",
    "# jnp.sum(output, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ac4f6",
   "metadata": {},
   "source": [
    "## Top2Gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52064340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\nThe error arose in jax.nn.one_hot argument `num_classes`.\nWhile tracing the function Top2Gating at /tmp/ipykernel_2310/2983666350.py:12 for jit, this value became a tracer due to JAX operations on these lines:\n\n  operation a:f32[] = ceil b\n    from line /tmp/ipykernel_2310/2983666350.py:7 (_capacity)\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConcretizationTypeError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2310/2983666350.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mTop2Gating_jit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTop2Gating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mcombine_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispatch_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTop2Gating_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2310/2983666350.py\u001b[0m in \u001b[0;36mTop2Gating\u001b[0;34m(gates)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mgates2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s,se->se\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgates2_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m#     print(gates1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mlocations1_sc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocations1_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapacity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mlocations2_sc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocations2_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapacity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mcombine1_sec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"se,sc->sec\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgates1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocations1_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/jax/_src/nn/functions.py\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(x, num_classes, dtype, axis)\u001b[0m\n\u001b[1;32m    358\u001b[0m       \u001b[0mcomputed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m   \"\"\"\n\u001b[0;32m--> 360\u001b[0;31m   num_classes = core.concrete_or_error(\n\u001b[0m\u001b[1;32m    361\u001b[0m       \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m       \"The error arose in jax.nn.one_hot argument `num_classes`.\")\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/jax/core.py\u001b[0m in \u001b[0;36mconcrete_or_error\u001b[0;34m(force, val, context)\u001b[0m\n\u001b[1;32m   1055\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mConcretizationTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConcretizationTypeError\u001b[0m: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\nThe error arose in jax.nn.one_hot argument `num_classes`.\nWhile tracing the function Top2Gating at /tmp/ipykernel_2310/2983666350.py:12 for jit, this value became a tracer due to JAX operations on these lines:\n\n  operation a:f32[] = ceil b\n    from line /tmp/ipykernel_2310/2983666350.py:7 (_capacity)\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "## combine_weights , dispatch_mask = Top2Gating ( gates )\n",
    "\n",
    "def _capacity(gates, capacity_factor, min_capacity=0):\n",
    "    # gates with shape SE as above\n",
    "    num_tokens = gates.shape[0]\n",
    "    num_experts = gates.shape[1]\n",
    "    capacity = jnp.ceil((num_tokens / num_experts) * capacity_factor)\n",
    "#     if capacity < min_capacity:\n",
    "#         capacity = min_capacity\n",
    "    return capacity\n",
    "\n",
    "def Top2Gating(gates):\n",
    "    indices1_s = jnp.argmax(gates, axis=1)\n",
    "#     print(\"top 1 gate id: {}\".format(indices1_s))\n",
    "    weights1 = jnp.array([row[indices1_s[i]] for i, row in enumerate(gates)])\n",
    "#     print(\"top 1 gate prob: {}\".format(weights1))\n",
    "    num_experts = gates.shape[1]\n",
    "    mask1 = jax.nn.one_hot(indices1_s, num_experts)\n",
    "#     print(\"one hot encoding mask for top 1 gates: {}\".format(mask1))\n",
    "    exp_counts1 = jnp.sum(mask1, axis=0)\n",
    "#     print(\"expert activations count: {}\".format(exp_counts1))\n",
    "    \n",
    "    # replace top experts with mean value\n",
    "    masked_gates2 = gates * (-mask1 + 1)\n",
    "#     print(\"masking out the largest prob for top 1: {}\".format(masked_gates2))\n",
    "    \n",
    "    indices2_s = jnp.argmax(masked_gates2, axis=1)\n",
    "    mask2 = jax.nn.one_hot(indices2_s, num_experts)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compute locations in capacity buffer\n",
    "    locations1 = jnp.cumsum(mask1, axis=0) - 1\n",
    "    locations2 = jnp.cumsum(mask2, axis=0) - 1\n",
    "#     print(\"locations2 in capacity buffer:{}\".format(locations2))\n",
    "    # Update 2nd's location by accounting for locations of 1st\n",
    "    locations2 += exp_counts1\n",
    "#     print(\"locations2 in capacity buffer:{}\".format(locations2))\n",
    "    \n",
    "    # Remove locations outside capacity from mask\n",
    "    capacity = _capacity(gates, capacity_factor=1.2)\n",
    "    \n",
    "#     print(\"one hot encoding mask for top 1 gates: {}\".format(mask1))\n",
    "    mask1 *= (locations1 < capacity)\n",
    "#     print(\"one hot encoding mask for top 1 gates after masking: {}\".format(mask1))\n",
    "    mask2 *= (locations2 < capacity)\n",
    "#     print(\"one hot encoding mask for top 2 gates after masking: {}\".format(mask2))\n",
    "\n",
    "    # Store the capacity location for each token\n",
    "    locations1_s = jnp.sum(locations1 * mask1, axis=1)\n",
    "#     print(\"capacity location1 for each token: {}\".format(locations1_s))\n",
    "    \n",
    "    locations2_s = jnp.sum(locations2 * mask2, axis=1)\n",
    "#     print(\"capacity location2 for each token: {}\".format(locations2_s))\n",
    "    \n",
    "    # normalize gate probs\n",
    "    gates1_s = jnp.einsum(\"se,se->s\", gates, mask1)\n",
    "    gates2_s = jnp.einsum(\"se,se->s\", gates, mask2)\n",
    "    denom_s = gates1_s + gates2_s\n",
    "    # avoid divide by 0\n",
    "    gates1_s /= (denom_s + 0.0001)\n",
    "    gates2_s /= (denom_s + 0.0001)\n",
    "    \n",
    "    # get combine_weights and dispatch_mask\n",
    "    gates1 = jnp.einsum(\"s,se->se\", gates1_s, mask1)\n",
    "    gates2 = jnp.einsum(\"s,se->se\", gates2_s, mask2)\n",
    "#     print(gates1)\n",
    "    locations1_sc = jax.nn.one_hot(locations1_s, capacity)\n",
    "    locations2_sc = jax.nn.one_hot(locations2_s, capacity)\n",
    "    combine1_sec = jnp.einsum(\"se,sc->sec\", gates1, locations1_sc)\n",
    "    combine2_sec = jnp.einsum(\"se,sc->sec\", gates2, locations2_sc)\n",
    "    combine_weights = combine1_sec + combine2_sec\n",
    "    print(combine_weights)\n",
    "    dispatch_mask = jnp.where(combine_weights > 0, 1, 0)\n",
    "    print(dispatch_mask.shape)\n",
    "    return combine_weights, dispatch_mask\n",
    "    \n",
    "Top2Gating_jit = jax.jit(Top2Gating)\n",
    "combine_weights, dispatch_mask = Top2Gating_jit(gates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e55b80bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128)\n"
     ]
    }
   ],
   "source": [
    "##  i sp a t ch e d _e x p er t _ in p u ts = einsum (\n",
    "## \"GSEC ,GSM - >EGCM \" , dispatch_mask , reshaped_inputs )\n",
    "## convert to local: \"SEC ,SM - >ECM \"\n",
    "print(local_inputs.shape)\n",
    "dispatched_input = jnp.einsum(\"sec,sm->ecm\", dispatch_mask, local_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f4fc15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
